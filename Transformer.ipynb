{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46568db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16c3d78-eaec-4b3d-b801-15291bc831ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values detected in dataset: house2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m train_datasets:\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mprint_sample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m Train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m val_datasets:\n\u001b[1;32m    121\u001b[0m     print_sample_data(dataset_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Validation\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_datasets[dataset_name])\n",
      "Cell \u001b[0;32mIn[12], line 103\u001b[0m, in \u001b[0;36mprint_sample_data\u001b[0;34m(dataset_name, dataset, sample_index)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_sample_data\u001b[39m(dataset_name, dataset, sample_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m     past_features, future_energy, energy_min_max \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    105\u001b[0m     inversed_future_energy \u001b[38;5;241m=\u001b[39m inverse_minmax_scale(future_energy, \u001b[38;5;241m*\u001b[39menergy_min_max)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 58\u001b[0m, in \u001b[0;36mSlidingWindowsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     56\u001b[0m window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx: idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorizon]\n\u001b[1;32m     57\u001b[0m past_all_features \u001b[38;5;241m=\u001b[39m window[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size]\n\u001b[0;32m---> 58\u001b[0m future_energy \u001b[38;5;241m=\u001b[39m \u001b[43mwindow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m scaled_past_all_features \u001b[38;5;241m=\u001b[39m (past_all_features \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_mins) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_maxs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_mins)\n\u001b[1;32m     62\u001b[0m scaled_future_energy, energy_min, energy_max \u001b[38;5;241m=\u001b[39m minmax_scale(future_energy)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import DataLoader\n",
    "from openpyxl import Workbook\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * torch.mean(2 * torch.abs(y_true - y_pred) / (torch.abs(y_true) + torch.abs(y_pred))).item()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "def mae(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "def minmax_scale(tensor):\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    scaled_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "    return scaled_tensor, tensor_min, tensor_max\n",
    "\n",
    "def inverse_minmax_scale(scaled_tensor, tensor_min, tensor_max):\n",
    "    original_tensor = scaled_tensor * (tensor_max - tensor_min) + tensor_min\n",
    "    return original_tensor\n",
    "\n",
    "window_size = 24\n",
    "horizon = 24\n",
    "train_prop = 0.6\n",
    "val_prop = 0.2\n",
    "\n",
    "df = pd.read_csv('datasets/usage/house1.csv')\n",
    "\n",
    "class SlidingWindowsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, window_size=24, horizon=24):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        self.feature_mins = self.data.min(axis=0).values\n",
    "        self.feature_maxs = self.data.max(axis=0).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.window_size + self.horizon) + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.data[idx: idx + self.window_size + self.horizon]\n",
    "        past_all_features = window[:self.window_size]\n",
    "        future_energy = window[self.window_size:, -1:]\n",
    "        \n",
    "        scaled_past_all_features = (past_all_features - self.feature_mins) / (self.feature_maxs - self.feature_mins)\n",
    "        \n",
    "        scaled_future_energy, energy_min, energy_max = minmax_scale(future_energy)\n",
    "\n",
    "        return scaled_past_all_features, scaled_future_energy, (energy_min, energy_max)\n",
    "\n",
    "df_dict = {\n",
    "    'house2': df,\n",
    "}\n",
    "\n",
    "train_datasets = {}\n",
    "val_datasets = {}\n",
    "test_datasets = {}\n",
    "\n",
    "for dataset_name, dataset_df in df_dict.items():\n",
    "    data_tensor = torch.from_numpy(dataset_df['PJME_MW'].values).float()\n",
    "    data_shape = data_tensor.shape[0]\n",
    "    train_end = int(data_shape * train_prop)\n",
    "    val_end = int(data_shape * (train_prop + val_prop))\n",
    "    train_data = data_tensor[:train_end]\n",
    "    val_data = data_tensor[train_end:val_end]\n",
    "    test_data = data_tensor[val_end:]\n",
    "    train_datasets[dataset_name] = SlidingWindowsDataset(train_data, window_size, horizon)\n",
    "    val_datasets[dataset_name] = SlidingWindowsDataset(val_data, window_size, horizon)\n",
    "    test_datasets[dataset_name] = SlidingWindowsDataset(test_data, window_size, horizon)\n",
    "\n",
    "def investigate_nan_in_dataframe(df, dataset_name):\n",
    "    if df.isnull().any().any():\n",
    "        print(f\"NaN values detected in dataset: {dataset_name}\")\n",
    "        \n",
    "        nan_columns = df.columns[df.isnull().any()].tolist()\n",
    "        print(f\"Columns with NaN values: {nan_columns}\")\n",
    "        \n",
    "        for col in nan_columns:\n",
    "            nan_rows = df.index[df[col].isnull()].tolist()\n",
    "            print(f\"Rows with NaN values in column {col}: {nan_rows}\")\n",
    "    else:\n",
    "        print(f\"No NaN values detected in dataset: {dataset_name}\")\n",
    "\n",
    "for dataset_name, dataset_df in df_dict.items():\n",
    "    investigate_nan_in_dataframe(dataset_df, dataset_name)\n",
    "\n",
    "def print_sample_data(dataset_name, dataset, sample_index=0):\n",
    "    past_features, future_energy, energy_min_max = dataset[sample_index]\n",
    "    \n",
    "    inversed_future_energy = inverse_minmax_scale(future_energy, *energy_min_max)\n",
    "    \n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(\"Past 24 hours' features:\")\n",
    "    print(past_features)\n",
    "    print(\"\\nNext 24 hours' scaled energy values:\")\n",
    "    print(future_energy)\n",
    "    print(\"\\nInverse scaled energy values for next 24 hours:\")\n",
    "    print(inversed_future_energy)\n",
    "    print(\"\\nMin-Max values used for scaling:\")\n",
    "    print(energy_min_max)\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "for dataset_name in train_datasets:\n",
    "    print_sample_data(dataset_name + \" Train\", train_datasets[dataset_name])\n",
    "\n",
    "for dataset_name in val_datasets:\n",
    "    print_sample_data(dataset_name + \" Validation\", val_datasets[dataset_name])\n",
    "\n",
    "for dataset_name in test_datasets:\n",
    "    print_sample_data(dataset_name + \" Test\", test_datasets[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdeb5565-0584-49da-af09-3dd7bfc9eb7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will be performed on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for dataset: house2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 199\u001b[0m\n\u001b[1;32m    196\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m    197\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m--> 199\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 144\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(datasets, num_epochs, patience, loss_fn)\u001b[0m\n\u001b[1;32m    141\u001b[0m val_smape_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 144\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     val_loss, val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#scheduler.step(val_loss)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     64\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMAPE\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m---> 67\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_energy_min_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_targets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaled_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfuture_energy_min_max\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[12], line 58\u001b[0m, in \u001b[0;36mSlidingWindowsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     56\u001b[0m window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx: idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorizon]\n\u001b[1;32m     57\u001b[0m past_all_features \u001b[38;5;241m=\u001b[39m window[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size]\n\u001b[0;32m---> 58\u001b[0m future_energy \u001b[38;5;241m=\u001b[39m \u001b[43mwindow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m scaled_past_all_features \u001b[38;5;241m=\u001b[39m (past_all_features \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_mins) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_maxs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_mins)\n\u001b[1;32m     62\u001b[0m scaled_future_energy, energy_min, energy_max \u001b[38;5;241m=\u001b[39m minmax_scale(future_energy)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# Transormer \"Attention All you Need\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(1, 0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "#Transformer without Decoder \n",
    "class TransformerWOD(nn.Module):\n",
    "    def __init__(self, num_features, d_model, nhead, num_layers):\n",
    "        super(TransformerWOD, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer.encoder(x)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "#Transformer with Decoder \n",
    "class TransformerWD(nn.Module):\n",
    "    def __init__(self, num_features, d_model, nhead, num_layers, num_decoder_layers=None):\n",
    "        super(TransformerWD, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        if num_decoder_layers is None:\n",
    "            num_decoder_layers = num_layers  \n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, num_decoder_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)  \n",
    "\n",
    "    def forward(self, src, tgt=None, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        if tgt is None:\n",
    "            tgt = src  \n",
    "        else:\n",
    "            tgt = self.embedding(tgt)\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "        memory = self.transformer.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.transformer.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=None, \n",
    "                                           tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(\"Training will be performed on GPU\")\n",
    "else:\n",
    "    print(\"Training will be performed on CPU\")\n",
    "learning_rate = 0.001\n",
    "def train(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    metrics = {'MAE': [], 'RMSE': [], 'SMAPE': []}\n",
    "\n",
    "    for inputs, scaled_targets, future_energy_min_max in data_loader:\n",
    "        inputs, scaled_targets = inputs.to(device), scaled_targets.to(device)\n",
    "        targets_min, targets_max = future_energy_min_max\n",
    "        targets_min, targets_max = targets_min.to(device), targets_max.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, scaled_targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        targets = inverse_minmax_scale(scaled_targets, targets_min, targets_max)\n",
    "        #print(\"Inverse Scaled Targets:\", targets[0].cpu().numpy())  # Print the first inverse scaled target\n",
    "        outputs = inverse_minmax_scale(outputs, targets_min, targets_max)\n",
    "        #print(\"Inverse Scaled Outputs:\", outputs[0].detach().cpu().numpy())    # Print the first inverse scaled output\n",
    "\n",
    "        metrics['MAE'].append(mae(outputs, targets).item())\n",
    "        metrics['RMSE'].append(rmse(outputs, targets).item())\n",
    "        metrics['SMAPE'].append(smape(outputs, targets))\n",
    "\n",
    "    return np.mean(losses), {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    metrics = {'MAE': [], 'RMSE': [], 'SMAPE': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, scaled_targets, future_energy_min_max in data_loader:\n",
    "            inputs, scaled_targets = inputs.to(device), scaled_targets.to(device)\n",
    "            targets_min, targets_max = future_energy_min_max\n",
    "            targets_min, targets_max = targets_min.to(device), targets_max.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, scaled_targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            targets = inverse_minmax_scale(scaled_targets, targets_min, targets_max)\n",
    "            outputs = inverse_minmax_scale(outputs, targets_min, targets_max)\n",
    "\n",
    "            metrics['MAE'].append(mae(outputs, targets).item())\n",
    "            metrics['RMSE'].append(rmse(outputs, targets).item())\n",
    "            metrics['SMAPE'].append(smape(outputs, targets))\n",
    "\n",
    "    return np.mean(losses), {k: np.mean(v) for k, v in metrics.items()}\n",
    "    \n",
    "\n",
    "model = TransformerWOD(num_features=6, d_model=128, nhead=4, num_layers=2).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "def train_and_validate(datasets, num_epochs, patience,loss_fn):\n",
    "    plots_dir = \"./Transformer_MSE/\"\n",
    "    \n",
    "    for dataset_name in datasets['train'].keys():\n",
    "        print(f\"Training for dataset: {dataset_name}\")\n",
    "        \n",
    "        train_loader = DataLoader(datasets['train'][dataset_name], batch_size=1, shuffle=False)\n",
    "        val_loader = DataLoader(datasets['val'][dataset_name], batch_size=1, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        train_mae_history = []\n",
    "        val_mae_history = []\n",
    "        train_rmse_history = []\n",
    "        val_rmse_history = []\n",
    "        train_smape_history = []\n",
    "        val_smape_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_metrics = train(model, train_loader, optimizer, loss_fn)\n",
    "            val_loss, val_metrics = evaluate(model, val_loader, loss_fn)\n",
    "            #scheduler.step(val_loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs for dataset {dataset_name}.\")\n",
    "                break\n",
    "\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            train_mae_history.append(train_metrics['MAE'])\n",
    "            val_mae_history.append(val_metrics['MAE'])\n",
    "            train_rmse_history.append(train_metrics['RMSE'])\n",
    "            val_rmse_history.append(val_metrics['RMSE'])\n",
    "            train_smape_history.append(train_metrics['SMAPE'])\n",
    "            val_smape_history.append(val_metrics['SMAPE'])\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} for dataset {dataset_name}\")\n",
    "            print(f\"Train Loss: {train_loss:.2f}, Validation Loss: {val_loss:.2f}\")\n",
    "            print(f\"Train MAE: {train_metrics['MAE']:.2f}, Validation MAE: {val_metrics['MAE']:.2f}\")\n",
    "            print(f\"Train RMSE: {train_metrics['RMSE']:.2f}, Validation RMSE: {val_metrics['RMSE']:.2f}\")\n",
    "            print(f\"Train SMAPE: {train_metrics['SMAPE']:.2f}, Validation SMAPE: {val_metrics['SMAPE']:.2f}\")\n",
    "\n",
    "        # Create plots and save them for each dataset\n",
    "        def plot_metrics(history1, history2, title, ylabel, filename):\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(y=history1, mode='lines', name='Train'))\n",
    "            fig.add_trace(go.Scatter(y=history2, mode='lines', name='Validation'))\n",
    "            fig.update_layout(title=title, xaxis_title='Epoch', yaxis_title=ylabel)\n",
    "            fig.write_image(f\"{plots_dir}/{dataset_name}_{filename}\")\n",
    "\n",
    "        plot_metrics(train_loss_history, val_loss_history, 'Loss vs Epoch', 'Loss', 'drop_loss_plot.png')\n",
    "        plot_metrics(train_mae_history, val_mae_history, 'MAE vs Epoch', 'MAE', 'drop_mae_plot.png')\n",
    "        plot_metrics(train_rmse_history, val_rmse_history, 'RMSE vs Epoch', 'RMSE', 'drop_rmse_plot.png')\n",
    "        plot_metrics(train_smape_history, val_smape_history, 'SMAPE vs Epoch', 'SMAPE', 'drop_smape_plot.png')\n",
    "\n",
    "        # Save the model for each dataset\n",
    "        torch.save(model.state_dict(), f'./Transformer_MSE/Transformer_{loss_fn}_{dataset_name}.pth')\n",
    "\n",
    "datasets = {\n",
    "    'train': train_datasets,\n",
    "    'val': val_datasets,\n",
    "    'test': test_datasets\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "patience = 7\n",
    "num_epochs = 100\n",
    "\n",
    "train_and_validate(datasets, num_epochs, patience,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fe162-030a-4e00-a8b7-b69f410dc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    metrics = {'MAE': [], 'RMSE': [], 'SMAPE': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, scaled_targets, future_energy_min_max in data_loader:\n",
    "            inputs, scaled_targets = inputs.to(device), scaled_targets.to(device)\n",
    "            targets_min, targets_max = future_energy_min_max\n",
    "            targets_min, targets_max = targets_min.to(device), targets_max.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, scaled_targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            targets = inverse_minmax_scale(scaled_targets, targets_min, targets_max)\n",
    "            outputs = inverse_minmax_scale(outputs, targets_min, targets_max)\n",
    "\n",
    "            metrics['MAE'].append(mae(outputs, targets).item())\n",
    "            metrics['RMSE'].append(rmse(outputs, targets).item())\n",
    "            metrics['SMAPE'].append(smape(outputs, targets))\n",
    "\n",
    "    return np.mean(losses), {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "# Load saved models and test\n",
    "test_results = {}\n",
    "for dataset_name in datasets['test'].keys():\n",
    "    print(f\"Testing for dataset: {dataset_name}\")\n",
    "    model.load_state_dict(torch.load(f'./Transformer_MSE/Transformer_{loss_fn}_{dataset_name}.pth'))\n",
    "    test_loader = DataLoader(datasets['test'][dataset_name], batch_size=1, shuffle=False)\n",
    "\n",
    "    test_loss, test_metrics = test(model, test_loader, loss_fn)\n",
    "    \n",
    "    print(f\"Test Loss for {dataset_name}: {test_loss:.2f}\")\n",
    "    print(f\"Test MAE for {dataset_name}: {test_metrics['MAE']:.2f}\")\n",
    "    print(f\"Test RMSE for {dataset_name}: {test_metrics['RMSE']:.2f}\")\n",
    "    print(f\"Test SMAPE for {dataset_name}: {test_metrics['SMAPE']:.2f}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    test_results[dataset_name] = {\n",
    "        'loss': test_loss,\n",
    "        'MAE': test_metrics['MAE'],\n",
    "        'RMSE': test_metrics['RMSE'],\n",
    "        'SMAPE': test_metrics['SMAPE']\n",
    "    }\n",
    "\n",
    "def test_with_values(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, scaled_targets, future_energy_min_max in data_loader:\n",
    "            inputs, scaled_targets = inputs.to(device), scaled_targets.to(device)\n",
    "            targets_min, targets_max = future_energy_min_max\n",
    "            targets_min, targets_max = targets_min.to(device), targets_max.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            targets = inverse_minmax_scale(scaled_targets, targets_min, targets_max).cpu().numpy()\n",
    "            outputs = inverse_minmax_scale(outputs, targets_min, targets_max).cpu().numpy()\n",
    "\n",
    "            all_outputs.extend(outputs)\n",
    "            all_targets.extend(targets)\n",
    "        \n",
    "    return all_targets, all_outputs\n",
    "\n",
    "# Load saved models, test and save to CSV\n",
    "for dataset_name in datasets['test'].keys():\n",
    "    print(f\"Generating values for dataset: {dataset_name}\")\n",
    "    model.load_state_dict(torch.load(f'./Transformer_MSE/Transformer_{loss_fn}_{dataset_name}.pth'))\n",
    "\n",
    "    test_loader = DataLoader(datasets['test'][dataset_name], batch_size=1, shuffle=False)\n",
    "\n",
    "    actual_values, predicted_values = test_with_values(model, test_loader, loss_fn)\n",
    "    print(len(np.ravel(actual_values)))\n",
    "    print(len(np.ravel(predicted_values)))\n",
    "    \n",
    "    '''\n",
    "    df_results = pd.DataFrame({\n",
    "        'Actual': actual_values,\n",
    "        'Predicted': predicted_values\n",
    "    })\n",
    "    '''\n",
    "    df_results = pd.DataFrame({\n",
    "        'Actual': np.ravel(actual_values),\n",
    "        'Predicted': np.ravel(predicted_values)\n",
    "    })\n",
    "    df_results.to_csv(f\"./Transformer_MSE/{dataset_name}_results.csv\", index=False)\n",
    "    print(f\"Saved results for {dataset_name} to {dataset_name}_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac75b1d-2a7a-4fca-bd24-c1afe5fbc380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
